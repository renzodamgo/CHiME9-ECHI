import math
from typing import Tuple, Any

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import init
from torch.nn.parameter import Parameter

HALF_PRECISION_DTYPES: tuple[Any, ...]
if hasattr(torch, "bfloat16"):
    HALF_PRECISION_DTYPES = (torch.float16, torch.bfloat16)
else:
    HALF_PRECISION_DTYPES = (torch.float16,)


class MCxTFGridNet(nn.Module):
    """Online TFGridNetV3.

    Adapted from:
        https://github.com/HaoFengyuan/X-TF-GridNet/blob/main/nnet/pTFGridNet.py
        https://github.com/espnet/espnet/blob/master/espnet2/enh/separator/tfgridnetv3_separator.py

    Reference:
    [1] Z.-Q. Wang, S. Cornell, S. Choi, Y. Lee, B.-Y. Kim, and S. Watanabe,
    "TF-GridNet: Integrating Full- and Sub-Band Modeling for Speech Separation",
    in TASLP, 2023.
    [2] Z.-Q. Wang, S. Cornell, S. Choi, Y. Lee, B.-Y. Kim, and S. Watanabe,
    "TF-GridNet: Making Time-Frequency Domain Models Great Again for Monaural
    Speaker Separation", in ICASSP, 2023.
    [3] Fengyuan Hao, Xiaodong Li, Chengshi Zheng,
    "X-TF-GridNet: A time–frequency domain target speaker extraction network with
    adaptive speaker embedding fusion", in Information Fusion, 2024.

    Args:
        n_srcs: number of output sources/speakers.
        n_fft: stft window size.
        stride: stft stride.
        window: stft window type choose between 'hamming', 'hanning' or None.
        n_imics: number of microphones channels (only fixed-array geometry supported).
        n_layers: number of TFGridNetV3 blocks.
        lstm_hidden_units: number of hidden units in LSTM.
        attn_n_head: number of heads in self-attention
        attn_attn_qk_output_channel: output channels of point-wise conv2d for getting
            key and query
        emb_dim: embedding dimension
        emb_ks: kernel size for unfolding and deconv1D
        emb_hs: hop size for unfolding and deconv1D
        activation: activation function to use in the whole TFGridNetV3 model,
            you can use any torch supported activation e.g. 'relu' or 'elu'.
        eps: small epsilon for normalization layers.
        use_builtin_complex: whether to use builtin complex type or not.
    """

    def __init__(
        self,
        n_srcs=2,
        n_imics=1,
        n_layers=6,
        lstm_hidden_units=192,
        attn_n_head=4,
        attn_qk_output_channel=4,
        emb_dim=48,
        emb_ks=4,
        emb_hs=1,
        activation="prelu",
        eps=1.0e-5,
    ):
        super().__init__()
        self.n_srcs = n_srcs
        print(f"n_srcs: {n_srcs} (number of output sources/speakers.)")
        self.n_layers = n_layers
        self.n_imics = n_imics

        t_ksize = 3
        ks, padding = (t_ksize, 3), (t_ksize // 2, 1)
        self.conv = nn.Sequential(
            nn.Conv2d(2 * n_imics, emb_dim, ks, padding=padding),
            LayerNormalization(emb_dim, eps=eps),
        )

        self.aux_enc = AuxEncoder(emb_dim, n_srcs)

        self.fusions = nn.ModuleList([])
        for _ in range(n_layers):
            self.fusions.append(FiLM(emb_dim, emb_dim))

        self.gridnets = nn.ModuleList([])
        for _ in range(n_layers):
            self.gridnets.append(
                GridNetV3Block(
                    emb_dim,
                    emb_ks,
                    emb_hs,
                    lstm_hidden_units,
                    n_head=attn_n_head,
                    qk_output_channel=attn_qk_output_channel,
                    activation=activation,
                    eps=eps,
                )
            )

        self.deconv = nn.ConvTranspose2d(emb_dim, n_srcs * 2, ks, padding=padding)

    def forward(self, spec: torch.Tensor, spk: torch.Tensor, spk_lens: torch.Tensor):
        """
        spec: [B, M, T, F, 2]  mixture (M = n_imics)
        spk : [B, T, F, 2] OR [B, K, T, F, 2] enrollment(s)
        spk_lens: [B] or [B, K] STFT frame lengths for enroll(s)

        Returns:
        if spk.ndim == 4 (single spk): [B, 1, n_srcs, T, F] complex  (backward compatible)
        if spk.ndim == 5 (K spk):      [B, K,        T, F] complex   (n_srcs is treated as 1 here)
        """
        assert spec.size(-1) == 2, spec.shape
        feature = spec.moveaxis(-1, 2)        # [B, M, 2, T, F] -> [B, M, 2, T, F] (no-op) then…
        feature = feature[:, :, 0]            # keep RI stacked in channel dim as in baseline
        n_batch, mics, _, n_frames, n_freqs = spec.moveaxis(-1, 2).shape
        assert mics == self.n_imics

        # --- Front-end ONCE on mixture ---
        if self.n_imics > 1:
            feat = feature.reshape(n_batch, 2 * mics, n_frames, n_freqs)
        else:
            feat = feature
        z_mix = self.conv(feat)               # [B, C, T, F]

        # --- Handle enrollments: single or K ---
        if spk.ndim == 4:
            # [B, T, F, 2] -> [B, 2, T, F] -> encode -> embedding [B, C]
            spk_feat = spk.moveaxis(-1, 1)
            spk_feat = self.conv(spk_feat)
            e, _ = self.aux_enc(spk_feat, spk_lens)

            z = z_mix
            for i in range(self.n_layers):
                z = self.fusions[i](e, z)
                z = self.gridnets[i](z)

            out = self.deconv(z)                               # [B, n_srcs*2, T, F]
            out = out.view(n_batch, self.n_srcs, 2, n_frames, n_freqs)
            out = torch.complex(out[:, :, 0], out[:, :, 1])    # [B, n_srcs, T, F]
            return out.unsqueeze(1)                            # [B, 1, n_srcs, T, F]  (compat)

        elif spk.ndim == 5:
            # [B, K, T, F, 2] -> flatten to [B*K, 2, T, F] for encoding
            B, K, T, F, _ = spk.shape
            spk_feat = spk.permute(0, 1, 4, 2, 3).reshape(B * K, 2, T, F)
            spk_feat = self.conv(spk_feat)

            # Lens: accept [B] or [B,K]
            if spk_lens.ndim == 1:             # same len for all K
                spk_lens = spk_lens.unsqueeze(1).expand(B, K).reshape(B * K)
            else:
                spk_lens = spk_lens.reshape(B * K)

            e, _ = self.aux_enc(spk_feat, spk_lens)           # [B*K, C]

            # Tile mixture features to K speakers: [B, C, T, F] -> [B*K, C, T, F]
            z = z_mix.unsqueeze(1).expand(B, K, *z_mix.shape[1:]).reshape(B * K, *z_mix.shape[1:])

            # Shared backbone, conditioned per-(B*K) stream
            for i in range(self.n_layers):
                z = self.fusions[i](e, z)
                z = self.gridnets[i](z)

            # One stream per speaker (set config/model.n_srcs=1 for joint training)
            out = self.deconv(z)                               # [B*K, 2, T, F] if n_srcs==1
            out = out.view(B, K, 2, n_frames, n_freqs)
            out = torch.complex(out[:, :, 0], out[:, :, 1])    # [B, K, T, F]
            return out

        else:
            raise ValueError(f"spk must be 4D or 5D, got {spk.ndim}")


    @property
    def num_spk(self):
        return self.n_srcs


class GridNetV3Block(nn.Module):
    def __getitem__(self, key):
        return getattr(self, key)

    def __init__(
        self,
        emb_dim,
        emb_ks,
        emb_hs,
        hidden_channels,
        n_head=4,
        qk_output_channel=4,
        activation="prelu",
        eps=1e-5,
    ):
        super().__init__()
        assert activation == "prelu"
        activation_fn = torch.nn.PReLU()

        in_channels = emb_dim * emb_ks

        self.intra_norm = nn.LayerNorm(emb_dim, eps=eps)
        self.intra_rnn = nn.LSTM(
            in_channels, hidden_channels, 1, batch_first=True, bidirectional=True
        )
        if emb_ks == emb_hs:
            self.intra_linear = nn.Linear(hidden_channels * 2, in_channels)
        else:
            self.intra_linear = nn.ConvTranspose1d(
                hidden_channels * 2, emb_dim, emb_ks, stride=emb_hs
            )

        self.inter_norm = nn.LayerNorm(emb_dim, eps=eps)
        self.inter_rnn = nn.LSTM(
            in_channels, hidden_channels, 1, batch_first=True, bidirectional=False
        )
        if emb_ks == emb_hs:
            self.inter_linear = nn.Linear(hidden_channels * 2, in_channels)
        else:
            self.inter_linear = nn.ConvTranspose1d(
                hidden_channels, emb_dim, emb_ks, stride=emb_hs
            )

        # use constant E not to be dependent on the number of frequency bins
        E = qk_output_channel
        assert emb_dim % n_head == 0

        self.add_module("attn_conv_Q", nn.Conv2d(emb_dim, n_head * E, 1))
        self.add_module(
            "attn_norm_Q",
            AllHeadPReLULayerNormalization4DC((n_head, E), eps=eps),
        )

        self.add_module("attn_conv_K", nn.Conv2d(emb_dim, n_head * E, 1))
        self.add_module(
            "attn_norm_K",
            AllHeadPReLULayerNormalization4DC((n_head, E), eps=eps),
        )

        self.add_module(
            "attn_conv_V", nn.Conv2d(emb_dim, n_head * emb_dim // n_head, 1)
        )
        self.add_module(
            "attn_norm_V",
            AllHeadPReLULayerNormalization4DC((n_head, emb_dim // n_head), eps=eps),
        )

        self.add_module(
            "attn_concat_proj",
            nn.Sequential(
                nn.Conv2d(emb_dim, emb_dim, 1),
                activation_fn,
                LayerNormalization(emb_dim, dim=-3, total_dim=4, eps=eps),
            ),
        )

        self.emb_dim = emb_dim
        self.emb_ks = emb_ks
        self.emb_hs = emb_hs
        self.n_head = n_head

    def forward(self, x):
        """GridNetV2Block Forward.

        Args:
            x: [B, C, T, Q]
            out: [B, C, T, Q]
        """
        B, C, old_T, old_Q = x.shape

        olp = self.emb_ks - self.emb_hs
        T = (
            math.ceil((old_T + 2 * olp - self.emb_ks) / self.emb_hs) * self.emb_hs
            + self.emb_ks
        )
        Q = (
            math.ceil((old_Q + 2 * olp - self.emb_ks) / self.emb_hs) * self.emb_hs
            + self.emb_ks
        )

        x = x.permute(0, 2, 3, 1)  # [B, old_T, old_Q, C]
        x = F.pad(x, (0, 0, olp, Q - old_Q - olp, olp, T - old_T - olp))  # [B, T, Q, C]

        # intra RNN
        input_ = x
        intra_rnn = self.intra_norm(input_)  # [B, T, Q, C]
        if self.emb_ks == self.emb_hs:
            intra_rnn = intra_rnn.view([B * T, -1, self.emb_ks * C])  # [BT, Q//I, I*C]
            intra_rnn, _ = self.intra_rnn(intra_rnn)  # [BT, Q//I, H]
            intra_rnn = self.intra_linear(intra_rnn)  # [BT, Q//I, I*C]
            intra_rnn = intra_rnn.view([B, T, Q, C])
        else:
            intra_rnn = intra_rnn.view([B * T, Q, C])  # [BT, Q, C]
            intra_rnn = intra_rnn.transpose(1, 2)  # [BT, C, Q]
            intra_rnn = F.unfold(
                intra_rnn[..., None], (self.emb_ks, 1), stride=(self.emb_hs, 1)
            )  # [BT, C*I, -1]
            intra_rnn = intra_rnn.transpose(1, 2)  # [BT, -1, C*I]

            intra_rnn, _ = self.intra_rnn(intra_rnn)  # [BT, -1, H]

            intra_rnn = intra_rnn.transpose(1, 2)  # [BT, H, -1]
            intra_rnn = self.intra_linear(intra_rnn)  # [BT, C, Q]
            intra_rnn = intra_rnn.view([B, T, C, Q])
            intra_rnn = intra_rnn.transpose(-2, -1)  # [B, T, Q, C]
        intra_rnn = intra_rnn + input_  # [B, T, Q, C]

        intra_rnn = intra_rnn.transpose(1, 2)  # [B, Q, T, C]

        # inter RNN
        input_ = intra_rnn
        inter_rnn = self.inter_norm(input_)  # [B, Q, T, C]
        if self.emb_ks == self.emb_hs:
            inter_rnn = inter_rnn.view([B * Q, -1, self.emb_ks * C])  # [BQ, T//I, I*C]
            inter_rnn, _ = self.inter_rnn(inter_rnn)  # [BQ, T//I, H]
            inter_rnn = self.inter_linear(inter_rnn)  # [BQ, T//I, I*C]
            inter_rnn = inter_rnn.view([B, Q, T, C])
        else:
            inter_rnn = inter_rnn.view(B * Q, T, C)  # [BQ, T, C]
            inter_rnn = inter_rnn.transpose(1, 2)  # [BQ, C, T]
            inter_rnn = F.unfold(
                inter_rnn[..., None], (self.emb_ks, 1), stride=(self.emb_hs, 1)
            )  # [BQ, C*I, -1]
            inter_rnn = inter_rnn.transpose(1, 2)  # [BQ, -1, C*I]

            inter_rnn, _ = self.inter_rnn(inter_rnn)  # [BQ, -1, H]

            inter_rnn = inter_rnn.transpose(1, 2)  # [BQ, H, -1]
            inter_rnn = self.inter_linear(inter_rnn)  # [BQ, C, T]
            inter_rnn = inter_rnn.view([B, Q, C, T])
            inter_rnn = inter_rnn.transpose(-2, -1)  # [B, Q, T, C]
        inter_rnn = inter_rnn + input_  # [B, Q, T, C]

        inter_rnn = inter_rnn.permute(0, 3, 2, 1)  # [B, C, T, Q]

        inter_rnn = inter_rnn[..., olp : olp + old_T, olp : olp + old_Q]
        batch = inter_rnn

        Q = self["attn_norm_Q"](self["attn_conv_Q"](batch))  # [B, n_head, C, T, Q]
        K = self["attn_norm_K"](self["attn_conv_K"](batch))  # [B, n_head, C, T, Q]
        V = self["attn_norm_V"](self["attn_conv_V"](batch))  # [B, n_head, C, T, Q]
        Q = Q.view(-1, *Q.shape[2:])  # [B*n_head, C, T, Q]
        K = K.view(-1, *K.shape[2:])  # [B*n_head, C, T, Q]
        V = V.view(-1, *V.shape[2:])  # [B*n_head, C, T, Q]

        Q = Q.transpose(1, 2)
        Q = Q.flatten(start_dim=2)  # [B', T, C*Q]

        K = K.transpose(2, 3)
        K = K.contiguous().view([B * self.n_head, -1, old_T])  # [B', C*Q, T]

        V = V.transpose(1, 2)  # [B', T, C, Q]
        old_shape = V.shape
        V = V.flatten(start_dim=2)  # [B', T, C*Q]
        emb_dim = Q.shape[-1]

        attn_mat = torch.matmul(Q, K) / (emb_dim**0.5)  # [B', T, T]

        causal_mask = (
            torch.tril(torch.ones(attn_mat.shape[-1], attn_mat.shape[-1]))
            .bool()
            .to(attn_mat.device)
        )
        attn_mat = attn_mat.masked_fill(~causal_mask, float("-inf"))

        attn_mat = F.softmax(attn_mat, dim=2)  # [B', T, T]
        V = torch.matmul(attn_mat, V)  # [B', T, C*Q]

        V = V.reshape(old_shape)  # [B', T, C, Q]
        V = V.transpose(1, 2)  # [B', C, T, Q]
        emb_dim = V.shape[1]

        batch = V.contiguous().view(
            [B, self.n_head * emb_dim, old_T, old_Q]
        )  # [B, C, T, Q])
        batch = self["attn_concat_proj"](batch)  # [B, C, T, Q])

        out = batch + inter_rnn
        return out


class LayerNormalization(nn.Module):
    def __init__(self, input_dim, dim=1, total_dim=4, eps=1e-5):
        super().__init__()
        self.dim = dim if dim >= 0 else total_dim + dim
        param_size = [1 if ii != self.dim else input_dim for ii in range(total_dim)]
        self.gamma = nn.Parameter(torch.Tensor(*param_size).to(torch.float32))
        self.beta = nn.Parameter(torch.Tensor(*param_size).to(torch.float32))
        nn.init.ones_(self.gamma)
        nn.init.zeros_(self.beta)
        self.eps = eps

    @torch.amp.autocast("cuda", enabled=False)
    def forward(self, x):
        if x.ndim - 1 < self.dim:
            raise ValueError(
                f"Expect x to have {self.dim + 1} dimensions, but got {x.ndim}"
            )
        if x.dtype in HALF_PRECISION_DTYPES:
            dtype = x.dtype
            x = x.float()
        else:
            dtype = None
        mu_ = x.mean(dim=self.dim, keepdim=True)
        std_ = torch.sqrt(x.var(dim=self.dim, unbiased=False, keepdim=True) + self.eps)
        x_hat = ((x - mu_) / std_) * self.gamma + self.beta
        return x_hat.to(dtype=dtype) if dtype else x_hat


class AllHeadPReLULayerNormalization4DC(nn.Module):
    def __init__(self, input_dimension, eps=1e-5):
        super().__init__()
        assert len(input_dimension) == 2, input_dimension
        H, E = input_dimension
        param_size = [1, H, E, 1, 1]
        self.gamma = Parameter(torch.Tensor(*param_size).to(torch.float32))
        self.beta = Parameter(torch.Tensor(*param_size).to(torch.float32))
        init.ones_(self.gamma)
        init.zeros_(self.beta)
        self.act = nn.PReLU(num_parameters=H, init=0.25)
        self.eps = eps
        self.H = H
        self.E = E

    def forward(self, x):
        assert x.ndim == 4
        B, _, T, F = x.shape
        x = x.view([B, self.H, self.E, T, F])
        x = self.act(x)  # [B,H,E,T,F]
        stat_dim = (2,)
        mu_ = x.mean(dim=stat_dim, keepdim=True)  # [B,H,1,T,1]
        std_ = torch.sqrt(
            x.var(dim=stat_dim, unbiased=False, keepdim=True) + self.eps
        )  # [B,H,1,T,1]
        x = ((x - mu_) / std_) * self.gamma + self.beta  # [B,H,E,T,F]
        return x


class AuxEncoder(nn.Module):
    def __init__(self, emb_dim, num_spks):
        super(AuxEncoder, self).__init__()
        k1, k2 = (1, 3), (1, 3)
        self.d_feat = emb_dim

        self.aux_enc = nn.ModuleList(
            [
                EnUnetModule(emb_dim, emb_dim, (1, 5), k2, scale=4),
                EnUnetModule(emb_dim, emb_dim, k1, k2, scale=3),
                EnUnetModule(emb_dim, emb_dim, k1, k2, scale=2),
                EnUnetModule(emb_dim, emb_dim, k1, k2, scale=1),
            ]
        )
        self.out_conv = nn.Linear(emb_dim, emb_dim)
        self.speaker = nn.Linear(emb_dim, num_spks)

    def forward(
        self, auxs: torch.Tensor, aux_lengths: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor]:

        aux_lengths = (((aux_lengths // 3) // 3) // 3) // 3
        auxs = auxs.transpose(2, 3)
        for i in range(len(self.aux_enc)):
            auxs = self.aux_enc[i](auxs)  # [B, C, T, F]

        auxs = torch.stack(
            [
                torch.mean(aux[:, :aux_length, :], dim=(1, 2))
                for aux, aux_length in zip(auxs, aux_lengths)
            ],
            dim=0,
        )  # [B, C]
        auxs = self.out_conv(auxs)
        return auxs, self.speaker(auxs)


class FiLM(nn.Module):
    def __init__(self, feature_dim, cond_dim):
        super(FiLM, self).__init__()
        self.gamma_fc = nn.Linear(cond_dim, feature_dim)
        self.beta_fc = nn.Linear(cond_dim, feature_dim)

    def forward(self, cond, x):
        """
        x:    [B, C, T, F] or [B, C]
        cond: [B, cond_dim]
        """
        gamma = self.gamma_fc(cond).unsqueeze(-1).unsqueeze(-1)  # [B, C, 1, 1]
        beta = self.beta_fc(cond).unsqueeze(-1).unsqueeze(-1)  # [B, C, 1, 1]
        return gamma * x + beta


class FusionModule(nn.Module):
    def __init__(self, emb_dim, nhead=4, dropout=0.1):
        super(FusionModule, self).__init__()
        self.nhead = nhead
        self.dropout = dropout
        param_size = [1, 1, emb_dim]

        self.attn = nn.MultiheadAttention(
            emb_dim, num_heads=nhead, dropout=dropout, batch_first=True
        )
        self.fusion = nn.Conv2d(emb_dim * 2, emb_dim, kernel_size=1)
        self.alpha = Parameter(torch.Tensor(*param_size).to(torch.float32))

        nn.init.zeros_(self.alpha)

    def forward(self, aux: torch.Tensor, esti: torch.Tensor) -> torch.Tensor:
        B, C, F, T = esti.shape

        aux = aux.unsqueeze(1)  # [B, 1, C]
        flatten_esti = esti.flatten(start_dim=2).transpose(1, 2)  # [B, T*F, C]
        # flatten_esti = esti

        aux_adapt = self.attn(aux, flatten_esti, flatten_esti, need_weights=False)[0]
        aux = aux + self.alpha * aux_adapt  # [B, 1, C]

        aux = aux.unsqueeze(-1).transpose(1, 2).expand_as(esti)
        esti = self.fusion(torch.cat((esti, aux), dim=1))  # [B, C, T, F]

        return esti


class EnUnetModule(nn.Module):
    def __init__(self, cin: int, cout: int, k1: tuple, k2: tuple, scale: int):
        super(EnUnetModule, self).__init__()
        self.k1 = k1
        self.k2 = k2
        self.cin = cin
        self.cout = cout
        self.scale = scale

        self.in_conv = nn.Sequential(
            GateConv2d(cin, cout, k1, (1, 2)), nn.BatchNorm2d(cout), nn.PReLU(cout)
        )
        self.encoder = nn.ModuleList([Conv2dUnit(k2, cout) for _ in range(scale)])
        self.decoder = nn.ModuleList([Deconv2dUnit(k2, cout, 1)])
        for i in range(1, scale):
            self.decoder.append(Deconv2dUnit(k2, cout, 2))
        self.out_pool = nn.AvgPool2d((3, 1))

    def forward(self, x: torch.Tensor):
        x_resi = self.in_conv(x)
        x = x_resi
        x_list = []
        for i in range(len(self.encoder)):
            x = self.encoder[i](x)
            x_list.append(x)
        x = self.decoder[0](x)
        for i in range(1, len(self.decoder)):

            x = self.decoder[i](torch.cat([x, x_list[-(i + 1)]], dim=1))

        x_resi = x_resi + x

        return self.out_pool(x_resi)


class GateConv2d(nn.Module):
    def __init__(self, cin: int, cout: int, k: tuple, s: tuple):
        super(GateConv2d, self).__init__()
        self.cin = cin
        self.cout = cout
        self.k = k
        self.s = s

        self.conv = nn.Sequential(
            nn.ConstantPad2d((0, 0, k[0] - 1, 0), value=0.0),
            nn.Conv2d(in_channels=cin, out_channels=cout * 2, kernel_size=k, stride=s),
        )

    def forward(self, inputs: torch.Tensor) -> torch.Tensor:
        x = self.conv(inputs)
        outputs, gate = x.chunk(2, dim=1)

        return outputs * gate.sigmoid()


class Conv2dUnit(nn.Module):
    def __init__(self, k: tuple, c: int):
        super(Conv2dUnit, self).__init__()
        self.k = k
        self.c = c
        self.conv = nn.Sequential(
            nn.Conv2d(c, c, k, (1, 2)), nn.BatchNorm2d(c), nn.PReLU(c)
        )

    def forward(self, x):
        return self.conv(x)


class Deconv2dUnit(nn.Module):
    def __init__(self, k: tuple, c: int, expend_scale: int):
        super(Deconv2dUnit, self).__init__()
        self.k = k
        self.c = c
        self.expend_scale = expend_scale
        self.deconv = nn.Sequential(
            nn.ConvTranspose2d(c * expend_scale, c, k, (1, 2)),
            nn.BatchNorm2d(c),
            nn.PReLU(c),
        )

    def forward(self, x):
        return self.deconv(x)


class LayerNormalization4D(nn.Module):
    def __init__(self, input_dimension, eps=1e-5):
        super().__init__()
        self.eps = eps

        param_size = [1, input_dimension, 1, 1]
        self.gamma = Parameter(torch.Tensor(*param_size).to(torch.float32))
        self.beta = Parameter(torch.Tensor(*param_size).to(torch.float32))

        nn.init.ones_(self.gamma)
        nn.init.zeros_(self.beta)

    def forward(self, x):
        if x.ndim == 4:
            stat_dim = (1,)
        else:
            raise ValueError("Expect x to have 4 dimensions, but got {}".format(x.ndim))

        mu_ = x.mean(dim=stat_dim, keepdim=True)  # [B,1,T,F]
        std_ = torch.sqrt(
            x.var(dim=stat_dim, unbiased=False, keepdim=True) + self.eps
        )  # [B,1,T,F]
        x_hat = ((x - mu_) / (std_ + self.eps)) * self.gamma + self.beta

        return x_hat


class LayerNormalization4DCF(nn.Module):
    def __init__(self, input_dimension, eps=1e-5):
        super().__init__()
        assert len(input_dimension) == 2
        self.eps = eps

        param_size = [1, input_dimension[0], 1, input_dimension[1]]
        self.gamma = Parameter(torch.Tensor(*param_size).to(torch.float32))
        self.beta = Parameter(torch.Tensor(*param_size).to(torch.float32))

        nn.init.ones_(self.gamma)
        nn.init.zeros_(self.beta)

    def forward(self, x):
        if x.ndim == 4:
            stat_dim = (1, 3)
        else:
            raise ValueError("Expect x to have 4 dimensions, but got {}".format(x.ndim))

        mu_ = x.mean(dim=stat_dim, keepdim=True)  # [B,1,T,1]
        std_ = torch.sqrt(
            x.var(dim=stat_dim, unbiased=False, keepdim=True) + self.eps
        )  # [B,1,T,1]
        x_hat = ((x - mu_) / (std_ + self.eps)) * self.gamma + self.beta

        return x_hat


if __name__ == "__main__":
    model = MCxTFGridNet()
    audio = torch.rand(1, 65, 1000, 2)
    aux = torch.rand(1, 65, 1000, 2)
    aux_lens = torch.tensor([1000])
    out = model(audio, aux, aux_lens)
